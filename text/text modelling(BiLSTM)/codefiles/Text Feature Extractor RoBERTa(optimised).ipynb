{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ba265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done data loading\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoConfig,\n",
    ")\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import random\n",
    "from sklearn.utils import resample\n",
    "import openl3\n",
    "\n",
    "train=pd.read_csv(\"C:/Users/Administrator/Downloads/DAIC_train_3sp_sampled.csv\")\n",
    "test=pd.read_csv(\"C:/Users/Administrator/Downloads/DAIC_test_3sp.csv\")\n",
    "torch.manual_seed(101)\n",
    "\n",
    "class TextFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def extract_features(self, texts):\n",
    "        # Tokenize input texts\n",
    "        tokenized_texts = self.tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Move tensors to the appropriate device\n",
    "        input_ids = tokenized_texts['input_ids'].to(self.device)\n",
    "        #print(tokenized_texts['input_ids'].shape)\n",
    "        attention_mask = tokenized_texts['attention_mask'].to(self.device)\n",
    "        #print(input_ids.shape)\n",
    "        # Extract text features from BERT model\n",
    "        #with torch.no_grad():\n",
    "            #outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "            #print(outputs[0])\n",
    "            #print(type(outputs))\n",
    "            #text_features = outputs[0][:, 0, :].cpu().numpy()  # Use the [CLS] token embedding\n",
    "            # print(text_features)\n",
    "        #print(\"Text feat dim:\"+str(text_features))\n",
    "        return input_ids\n",
    "\n",
    "    \n",
    "class MultimodalDataset(Dataset):\n",
    "        def __init__(self, data_fr, transform_text=None):\n",
    "            self.data_fr = data_fr\n",
    "            self.transform_text = transform_text\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data_fr)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "        #print(audio_file)\n",
    "            text_p =  self.data_fr.loc[index, \"text\"]\n",
    "            class_id = self.data_fr.loc[index, \"Group\"]\n",
    "        #audio_file = self.audio_files[index]\n",
    "        #text = self.texts[index]\n",
    "\n",
    "\n",
    "        # Extract text features\n",
    "            text_features = self.transform_text.extract_features([text_p])[0]\n",
    "\n",
    "            return text_features, class_id\n",
    "\n",
    "    \n",
    "    \n",
    "batch_size = 32\n",
    "\n",
    "# Initialize feature extractor\n",
    "\n",
    "text_feature_extractor = TextFeatureExtractor()\n",
    "# Create dataset\n",
    "dataset_tr = MultimodalDataset(train, transform_text=text_feature_extractor)\n",
    "dataset_ts = MultimodalDataset(test, transform_text=text_feature_extractor)\n",
    "\n",
    "# Create dataloader\n",
    "tr_dataloader = DataLoader(dataset_tr, batch_size=batch_size, shuffle=True)\n",
    "ts_dataloader = DataLoader(dataset_ts, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Done data loading\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9917e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f15c3663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "[INFO] Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 321/321 [00:25<00:00, 12.56it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:04<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.67      0.69      1349\n",
      "           1       0.29      0.32      0.30       565\n",
      "\n",
      "    accuracy                           0.57      1914\n",
      "   macro avg       0.49      0.49      0.49      1914\n",
      "weighted avg       0.58      0.57      0.57      1914\n",
      "\n",
      "Train loss: 0.6953, Train accuracy:  0.5252\n",
      "Validation loss: 0.6781, Validation accuracy:  0.5664\n",
      "[INFO] Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 321/321 [00:28<00:00, 11.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:04<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.66      0.69      1349\n",
      "           1       0.30      0.35      0.32       565\n",
      "\n",
      "    accuracy                           0.57      1914\n",
      "   macro avg       0.50      0.50      0.50      1914\n",
      "weighted avg       0.59      0.57      0.58      1914\n",
      "\n",
      "Train loss: 0.6912, Train accuracy:  0.5355\n",
      "Validation loss: 0.6793, Validation accuracy:  0.5700\n",
      "[INFO] Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 321/321 [00:27<00:00, 11.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:03<00:00, 16.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.30      0.42      1349\n",
      "           1       0.30      0.72      0.43       565\n",
      "\n",
      "    accuracy                           0.42      1914\n",
      "   macro avg       0.51      0.51      0.42      1914\n",
      "weighted avg       0.60      0.42      0.42      1914\n",
      "\n",
      "Train loss: 0.6928, Train accuracy:  0.5300\n",
      "Validation loss: 0.7366, Validation accuracy:  0.4242\n",
      "[INFO] Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 321/321 [00:29<00:00, 10.77it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:03<00:00, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.23      0.35      1349\n",
      "           1       0.30      0.77      0.43       565\n",
      "\n",
      "    accuracy                           0.39      1914\n",
      "   macro avg       0.50      0.50      0.39      1914\n",
      "weighted avg       0.59      0.39      0.37      1914\n",
      "\n",
      "Train loss: 0.6925, Train accuracy:  0.5403\n",
      "Validation loss: 0.7521, Validation accuracy:  0.3929\n",
      "[INFO] Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 321/321 [00:28<00:00, 11.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:03<00:00, 16.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.43      0.53      1349\n",
      "           1       0.30      0.59      0.40       565\n",
      "\n",
      "    accuracy                           0.47      1914\n",
      "   macro avg       0.51      0.51      0.47      1914\n",
      "weighted avg       0.59      0.47      0.49      1914\n",
      "\n",
      "Train loss: 0.6919, Train accuracy:  0.5416\n",
      "Validation loss: 0.7152, Validation accuracy:  0.4744\n",
      "[INFO] Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 321/321 [00:30<00:00, 10.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:04<00:00, 14.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.33      0.45      1349\n",
      "           1       0.30      0.67      0.41       565\n",
      "\n",
      "    accuracy                           0.43      1914\n",
      "   macro avg       0.50      0.50      0.43      1914\n",
      "weighted avg       0.58      0.43      0.44      1914\n",
      "\n",
      "Train loss: 0.6884, Train accuracy:  0.5436\n",
      "Validation loss: 0.7372, Validation accuracy:  0.4300\n",
      "[INFO] Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 321/321 [00:26<00:00, 11.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:04<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.17      0.27      1349\n",
      "           1       0.30      0.85      0.44       565\n",
      "\n",
      "    accuracy                           0.37      1914\n",
      "   macro avg       0.51      0.51      0.36      1914\n",
      "weighted avg       0.60      0.37      0.32      1914\n",
      "\n",
      "Train loss: 0.6886, Train accuracy:  0.5397\n",
      "Validation loss: 0.7819, Validation accuracy:  0.3694\n",
      "[INFO] Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 321/321 [00:37<00:00,  8.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:04<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.36      0.47      1349\n",
      "           1       0.28      0.61      0.39       565\n",
      "\n",
      "    accuracy                           0.43      1914\n",
      "   macro avg       0.48      0.48      0.43      1914\n",
      "weighted avg       0.57      0.43      0.45      1914\n",
      "\n",
      "Train loss: 0.6898, Train accuracy:  0.5458\n",
      "Validation loss: 0.7310, Validation accuracy:  0.4316\n",
      "[INFO] Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 321/321 [00:25<00:00, 12.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:03<00:00, 18.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.68      0.69      1349\n",
      "           1       0.28      0.31      0.30       565\n",
      "\n",
      "    accuracy                           0.57      1914\n",
      "   macro avg       0.49      0.49      0.49      1914\n",
      "weighted avg       0.58      0.57      0.57      1914\n",
      "\n",
      "Train loss: 0.6896, Train accuracy:  0.5422\n",
      "Validation loss: 0.6795, Validation accuracy:  0.5674\n",
      "[INFO] Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 321/321 [00:25<00:00, 12.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:03<00:00, 18.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.64      0.67      1349\n",
      "           1       0.29      0.35      0.32       565\n",
      "\n",
      "    accuracy                           0.56      1914\n",
      "   macro avg       0.50      0.50      0.50      1914\n",
      "weighted avg       0.58      0.56      0.57      1914\n",
      "\n",
      "Train loss: 0.6878, Train accuracy:  0.5386\n",
      "Validation loss: 0.6866, Validation accuracy:  0.5569\n",
      "[INFO] Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 321/321 [00:25<00:00, 12.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:03<00:00, 18.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.49      0.58      1349\n",
      "           1       0.29      0.51      0.37       565\n",
      "\n",
      "    accuracy                           0.50      1914\n",
      "   macro avg       0.50      0.50      0.48      1914\n",
      "weighted avg       0.58      0.50      0.52      1914\n",
      "\n",
      "Train loss: 0.6881, Train accuracy:  0.5423\n",
      "Validation loss: 0.7076, Validation accuracy:  0.4953\n",
      "[INFO] Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 321/321 [00:25<00:00, 12.75it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:03<00:00, 17.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.71      1349\n",
      "           1       0.30      0.28      0.29       565\n",
      "\n",
      "    accuracy                           0.59      1914\n",
      "   macro avg       0.50      0.50      0.50      1914\n",
      "weighted avg       0.58      0.59      0.59      1914\n",
      "\n",
      "Train loss: 0.6899, Train accuracy:  0.5401\n",
      "Validation loss: 0.6685, Validation accuracy:  0.5920\n"
     ]
    }
   ],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    #propagation:\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        #print(\"After\")\n",
    "        #print(x.shape)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out\n",
    "\n",
    "class AccuracyMetric:\n",
    "    def __init__(self):\n",
    "        self.correct, self.total = None, None\n",
    "        self.reset()\n",
    "\n",
    "    def update(self, y_pred, y_true):\n",
    "        self.correct += torch.sum(y_pred.argmax(-1) == y_true).item()\n",
    "        self.total += y_true.size(0)\n",
    "\n",
    "    def compute(self):\n",
    "        return self.correct / self.total\n",
    "\n",
    "    def reset(self):\n",
    "        self.correct = 0\n",
    "        self.total = 0    \n",
    "\n",
    "# Set the hyperparameters\n",
    "#input_size = embeddings.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "model = LSTMClassifier(512, hidden_size, num_classes)\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = model.to(device)\n",
    "next(model.parameters()).device\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "epochs = 12\n",
    "\n",
    "# training loop\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "\n",
    "valid_loss_history = []\n",
    "valid_accuracy_history = []\n",
    "\n",
    "accuracy = AccuracyMetric()\n",
    "best_acc=0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"[INFO] Epoch: {epoch}\")\n",
    "    model.train()\n",
    "\n",
    "    batch_train_loss = []\n",
    "    batch_valid_loss = []\n",
    "\n",
    "    predicted_label=[]\n",
    "\n",
    "    true_label=[]\n",
    "    \n",
    "    for osfeat, y_batch in tqdm(tr_dataloader):\n",
    "        # perform single training step\n",
    "        #print(X_batch.shape)\n",
    "        #print(osfeat.shape)\n",
    "        model.zero_grad()\n",
    "        #print(\"Before\")\n",
    "        \n",
    "        #osfeat = true features(~ x_batch for test)\n",
    "        \n",
    "        osfeat, y_batch = osfeat.to(device), y_batch.to(device)\n",
    "        y_predicted = model(osfeat.float())\n",
    "        #print(\"After Model call\")\n",
    "        loss = criterion(y_predicted, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        accuracy.update(y_predicted, y_batch)\n",
    "        batch_train_loss.append(loss.item())\n",
    "\n",
    "    mean_epoch_loss_train = np.mean(batch_train_loss)\n",
    "    train_accuracy = accuracy.compute()\n",
    "\n",
    "    train_loss_history.append(mean_epoch_loss_train)\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "    accuracy.reset()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for osfeat, y_batch in tqdm(ts_dataloader):\n",
    "            #print(X_batch.shape)\n",
    "            true_label+=list(y_batch.tolist())\n",
    "            osfeat, y_batch = osfeat.to(device), y_batch.to(device)\n",
    "\n",
    "            y_predicted = model(osfeat.float())\n",
    "            #print(y_predicted)\n",
    "            loss_val = criterion(y_predicted, y_batch)\n",
    "            predicted_label+=y_predicted.argmax(-1).tolist()\n",
    "            accuracy.update(y_predicted, y_batch)\n",
    "            batch_valid_loss.append(loss_val.item())\n",
    "\n",
    "    mean_epoch_loss_valid = np.mean(batch_valid_loss)\n",
    "    valid_accuracy = accuracy.compute()\n",
    "\n",
    "    if valid_accuracy > best_acc:\n",
    "        best_acc=valid_accuracy\n",
    "        model_depression_text_LSTM_best_roberta = torch.jit.script(model)\n",
    "        model_depression_text_LSTM_best_roberta.save('model_depression_text_LSTM_best_roberta.pt')\n",
    "        \n",
    "\n",
    "\n",
    "    valid_loss_history.append(mean_epoch_loss_valid)\n",
    "    valid_accuracy_history.append(valid_accuracy)\n",
    "    accuracy.reset()\n",
    "    print(classification_report(true_label, predicted_label))\n",
    "    print(\n",
    "        f\"Train loss: {mean_epoch_loss_train:0.4f}, Train accuracy: {train_accuracy: 0.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Validation loss: {mean_epoch_loss_valid:0.4f}, Validation accuracy: {valid_accuracy: 0.4f}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
